---
layout : post
title : Horde - Learning Knowledge from Unsupervised Sensorimotor Interaction
date : 2018-07-05
time : +1924
permalink : posts/Horde
tags : summary
---

**Richard S. Sutton**, Joseph Modayil, Michael Delp, Thomas Degris, Patrick M. Pilarski, Adam White, Doina Precup
[[AAMAS 2011](https://www.cs.swarthmore.edu/~meeden/DevelopmentalRobotics/horde1.pdf)] 

---

### What?

- A value function asks a question — what will the cumulative future reward be? — and an approximate value function provides an answer to that question. The approximate value function is the knowledge, and its match to the value function—to the actual future reward — defines what it means for the knowledge to be accurate.
    - The idea of the present work is that the value-function approach to grounding semantics can be extended beyond reward to a theory of all world knowledge.
    - i.e. a conventional reward function provides a grounded semantics for knowledge about upcoming reward.

### How?

- General Value Functions (GVFs) provide a grounded semantics for a more general kind of world knowledge. The four functions, $$\pi$$, $$\gamma$$, r, and z, are referred to collectively as the GVF’s question functions; they define the question or semantics of the GVF.

$$q(s, a; \pi, \gamma, r, z) = E[G_t | S_t = s, A_t = a, A_{t+1:T −1} ∼ \pi, T ∼ \gamma]$$

- The Horde architecture consists of an overall agent composed of many sub-agents, called demons. Each demon is a independent reinforcement-learning agent responsible for learning one small piece of knowledge about the base agent’s interaction with its environment. Each demon learns an approximation, $$\hat{q}$$, to the GVF, $$q$$, that corresponds to the demon’s setting of the four question functions, $$\pi$$, $$\gamma$$, r, and z.
- Off-policy learning enables massive parallelization of learning about each of the different target policies $$\pi$$ of the demons while using a single behavioural policy.
- The authors use GQ($$\lambda$$), which is a recent algorithm that works reliably with function approximation and scales appropriately for real-time learning and prediction. The approximation that will be found asymptotically by the GQ($$\lambda$$) algorithm depends on the feature vectors $$\phi$$, the behavior policy $$b$$, and the eligibility-trace function $$\lambda$$. These three are collectively referred to as the answer functions.


### Experiments:

- They first show that a single (prediction) demon is able to correctly predict the time steps before hitting an obstacle.
- The second experiment involves eight control demons which successfully learn to maximise eight different objectives. Empirically, in 4x the time it takes for each of the demons to learn the expected behavior on-policy, they learn the behavior for 8 demons, off-policy.
- The final experiment demonstrates that demons can learn effective goal-directed behavior from substantially different training behavior.

### Future work:

- Learn to combine these 'bits' of information together, which could make this framework quite useful.